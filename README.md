# Bias-in-AI-essay-
A 1000 word essay discussing bias and discrimination in AI. 


This essay has a visionary approach based on the 20 scientific papers below. The first section is an analysis of a scientific paper on a topic and the second section discusses how discrimination-aware AI will develop in the future. 



In this essay I will review one reading and speak about how it relates to algorithmic bias, as well as the effects it has more broadly. After this I will discuss which direction I think we will go in with developing AI, and the steps we should take.  


In [1], it deals with many different areas of developing trust between AI predictive models and society. Whilst it states that ethical considerations such as human rights and non-discrimination approaches are “inevitably related to perceptions of trust they are not necessarily the same thing”, I still believe that the paper heavily relates to the scope of algorithmic bias. Firstly, it outlines the machine learning pipeline and how it works and suggests how trust can be built and broken in each stage of the pipeline, including in the implementation of the algorithm, indicating that an algorithm needs to be fair in order to trust it. It then outlines how trust is “a priori” and should start at the beginning of the development and so bias and discrimination should also be thought about at this time, before the algorithm is developed. Although it suggests that discrimination can stem from the original dataset and that we cannot “simply ignore or remove features associated with unfairness”, it also highlights how algorithmic bias is “at the heart of trust challenges” and thus the work in resolving bias will hopefully have the reward of social trust within the future. Through introducing a FEAS (Fairness, Explainability, Auditability, Safety) classification of machine learning technologies, the paper highlights how companies only optimise algorithms for efficiency and accuracy alone, and how the new technological qualities should include Fairness, Explainability, Auditability and Safety. The paper outlines the issues in measuring unfairness in algorithms as well as the difficulty in choosing metrics, due to the size of choice. It references “Zliobaite” [2], which identifies two conditions for non-discriminating. The fact it uses this as a reference shows how mitigating discrimination, achieving fair algorithms and building trust are intrinsically linked. It continues to talk about explainability and how most of the algorithms are black-box. Whilst the essay focuses on supporting explainability for building trust, the consequence of explainable outcomes will mean we can better detect algorithmic bias through being able to explain whether it is fair or not, as well as mitigate discrimination. The overall impact of this paper raises awareness of discrimination in AI and highlights some historic pitfall’s of trust including misuse of personal data like Cambridge Analytica. By convincing the reader to research more into trustworthy algorithms, it may spread word of the importance of trustworthy algorithms socially, causing companies to  reduce algorithmic bias and mitigate discrimination within their products.

I think that it is important for researchers like the authors of the reading above to continue their work. The importance of such research into trust, such as that in [3] or [4] highlights how temperamental developing fair AI will be in the future. With papers talking about developing a social license and improving public engagement in company activities, it raises awareness through their readers and thus in society. The more awareness raised means that major companies will place more emphasis on the research they do into their own algorithms and so will put more effort into reducing bias into the future. Throughout the increasing research that is being conducted such as in [5] and [6], it seems like the definition of “fairness” is still not agreed upon and the increasing number of metrics used makes measuring fairness difficult and subjective. I think that in the future as more research is conducted due to the rise in social awareness, these problems will only increase.  However, after reading [7], I became more confident in this not happening as it highlighted these issues.  The fact that most papers work with “issues in explainability, robustness and fairness are confined to their specific subfield” and that few tools including googles “what-if” allow developers to use simultaneously when building their pipelines in a fair way.  It suggested a meta-heuristic model that is model agnostic and works with different data types, as well as suggested the “first ever black-box model robustness score”. I think more papers like this will mean that fairness may not be limited to specific models and that bias can become more broadly mitigated which is in issue I previously thought would not be resolved. 
Lastly, in the future I think that AI will become far more ubiquitous. Whilst [8] suggests it will be used to “augment and streamline many human activities” and will affect employment, education, finance and even judgements in court, all this needs to be done fairly. After reading [9], it showed how it may even cause “nuclear escalation” in [10], I expect a great surge in research in bias mitigation in future, which may increase social engagement and the knowledge shared in companies, governments and other areas of society. The growth will become exponential, but the ability to trust AI socially will only come after the research becomes more conclusive, a social license is established in companies, and the risk of losing trust will remain constantly high. 


Bibliography
[1] “The relationship between trust in AI and trustworthy machine learning technologies”, Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel
[2] “Measuring discrimination in algorithmic decision making. Data Mining and Knowledge Discovery”, Indre Zliobaite\.e. 2017. 
[3] “Trusting Intelligent Machines: Deepening Trust Within SocioTechnical Systems”, Peter Andras et al., IEEE Technology and Society Magazine
[4] “Establishing a social licence for Financial Technology: Reflections on the role of private secor in pursing ethical data practices”, Ehsan Toreini, Mhairi Aitken, Peter Carmichael, Karen Elliott, Aad van Moorsel
[5] “Equality of Opportunity in Supervised Learning”, Moritz Hardt, Eric Price, Eric Price, Nati Srebro, 2016
[6] ”Conscientious classification: a data scientist’s guide to discrimination -aware classification”, Brian D’Alessandrio et al., 2017
[7] “A common framework to provide explanations and analyse the fairness and robustness of black-box models”, S Sharma, J Henderson, J Ghosh, AIES 2020
[8] “The impact of AI on business and society”, Lucy Colback, Financial Times, October 16, 2020
[9] “Artificial Intelligence: A Threat to Strategic Stability”, James S. Johnson
